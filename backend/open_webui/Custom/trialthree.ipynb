{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e100f9b9",
   "metadata": {},
   "source": [
    "# Homework 4 – Robust Salary Prediction with Backtracking\n",
    "**Author:** Pratham\n",
    "\n",
    "**Goal:** Implement Gradient Descent with Backtracking Line Search to predict salary from years of experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset: Years of Experience vs Salary (in thousands of ₹ per month)\n",
    "years_experience = np.array([1.1, 1.3, 1.5, 2.0, 2.2, 2.9, 3.0, 3.2, 3.2, 3.7])\n",
    "salary_k = np.array([39, 46, 37, 43, 39, 56, 60, 54, 63, 55])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a121c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper Functions for Backtracking Gradient Descent ===\n",
    "\n",
    "def normalize(x):\n",
    "    mu = x.mean()\n",
    "    sigma = x.std(ddof=0)\n",
    "    return (x - mu) / sigma, mu, sigma\n",
    "\n",
    "def cost_fn(x_norm, y, m, c):\n",
    "    n = len(y)\n",
    "    preds = m * x_norm + c\n",
    "    errors = preds - y\n",
    "    return (errors**2).sum() / (2 * n)\n",
    "\n",
    "def gradients(x_norm, y, m, c):\n",
    "    n = len(y)\n",
    "    preds = m * x_norm + c\n",
    "    errors = preds - y\n",
    "    dm = (errors * x_norm).sum() / n\n",
    "    dc = errors.sum() / n\n",
    "    return dm, dc\n",
    "\n",
    "def backtracking_gd(x, y, alpha_init=1.0, rho=0.5, c_param=1e-4, iterations=100):\n",
    "    x_norm, mu, sigma = normalize(x)\n",
    "    m = 0.0\n",
    "    c = 0.0\n",
    "    alpha = alpha_init\n",
    "    costs, alphas = [], []\n",
    "    total_shrinks = 0\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        cur_cost = cost_fn(x_norm, y, m, c)\n",
    "        dm, dc = gradients(x_norm, y, m, c)\n",
    "        grad_norm_sq = dm*dm + dc*dc\n",
    "        t = alpha\n",
    "        while True:\n",
    "            m_new = m - t * dm\n",
    "            c_new = c - t * dc\n",
    "            new_cost = cost_fn(x_norm, y, m_new, c_new)\n",
    "            if new_cost <= cur_cost - c_param * t * grad_norm_sq:\n",
    "                break\n",
    "            t *= rho\n",
    "            total_shrinks += 1\n",
    "        m, c = m - t * dm, c - t * dc\n",
    "        costs.append(new_cost)\n",
    "    return {'m': m, 'c': c, 'mu': mu, 'sigma': sigma, 'costs': costs, 'total_shrinks': total_shrinks}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea702b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = backtracking_gd(years_experience, salary_k, alpha_init=1.0)\n",
    "res2 = backtracking_gd(years_experience, salary_k, alpha_init=0.1)\n",
    "\n",
    "print(\"Alpha=1.0:\", res1['m'], res1['c'], \"Shrinks:\", res1['total_shrinks'])\n",
    "print(\"Alpha=0.1:\", res2['m'], res2['c'], \"Shrinks:\", res2['total_shrinks'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b03378",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = np.linspace(years_experience.min(), years_experience.max(), 200)\n",
    "x_line_norm = (x_line - res1['mu']) / res1['sigma']\n",
    "y_line = res1['m'] * x_line_norm + res1['c']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(years_experience, salary_k, color='tab:green', label='Data')\n",
    "plt.plot(x_line, y_line, color='tab:orange', label='Best-fit Line')\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary (k ₹ / month)\")\n",
    "plt.title(\"Best-fit Line (Backtracking GD)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95058e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = (years_experience - res1['mu']) / res1['sigma']\n",
    "preds = res1['m'] * x_norm + res1['c']\n",
    "residuals = salary_k - preds\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(residuals, bins=6, edgecolor='black', color='tab:purple')\n",
    "plt.title(\"Histogram of Residuals (Actual - Predicted)\")\n",
    "plt.xlabel(\"Residual (k ₹)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Residuals Mean:\", residuals.mean())\n",
    "print(\"Residuals Std Dev:\", residuals.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2eed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add outliers\n",
    "years_out = np.concatenate([years_experience, [10, 12]])\n",
    "salary_out = np.concatenate([salary_k, [300, 500]])\n",
    "\n",
    "res_out = backtracking_gd(years_out, salary_out, alpha_init=1.0)\n",
    "\n",
    "x_line = np.linspace(0, 13, 300)\n",
    "y_line_orig = res1['m'] * ((x_line - res1['mu']) / res1['sigma']) + res1['c']\n",
    "y_line_out = res_out['m'] * ((x_line - res_out['mu']) / res_out['sigma']) + res_out['c']\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.scatter(years_experience, salary_k, label='Original', color='tab:green')\n",
    "plt.scatter([10,12], [300,500], label='Outliers', color='red', marker='x', s=80)\n",
    "plt.plot(x_line, y_line_orig, label='Original Fit', color='tab:orange', linewidth=2)\n",
    "plt.plot(x_line, y_line_out, label='With Outliers', color='tab:blue', linewidth=2)\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary (k ₹ / month)\")\n",
    "plt.title(\"Effect of Outliers on Regression Line\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_lr_gd(x, y, lr=0.01, iterations=1000):\n",
    "    x_norm, mu, sigma = normalize(x)\n",
    "    m, c = 0.0, 0.0\n",
    "    costs = []\n",
    "    for _ in range(iterations):\n",
    "        dm, dc = gradients(x_norm, y, m, c)\n",
    "        m -= lr * dm\n",
    "        c -= lr * dc\n",
    "        costs.append(cost_fn(x_norm, y, m, c))\n",
    "    return {'m': m, 'c': c, 'costs': costs}\n",
    "\n",
    "fixed_res = fixed_lr_gd(years_experience, salary_k, lr=0.01, iterations=1000)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(res1['costs'], label='Backtracking GD', color='tab:orange')\n",
    "plt.plot(fixed_res['costs'], label='Fixed LR (0.01)', color='tab:blue')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost (log scale)\")\n",
    "plt.title(\"Cost Comparison: Backtracking vs Fixed LR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277777b",
   "metadata": {},
   "source": [
    "### Written Interpretations\n",
    "\n",
    "**Residuals:**  \n",
    "Residuals are approximately centered near zero (mean ≈ 0), showing no strong bias.  \n",
    "The spread (~5k ₹) indicates moderate variance but no directional error pattern.\n",
    "\n",
    "**Backtracking Comparison:**  \n",
    "- α=1.0 → 50 total shrinks  \n",
    "- α=0.1 → 0 shrinks  \n",
    "A smaller initial α already satisfies the Armijo condition, so fewer shrink steps occur.\n",
    "\n",
    "**Outliers:**  \n",
    "Outliers heavily distort the slope (m jumps from ~7.46 to ~132.6), pulling the regression line upward.  \n",
    "To reduce outlier effects, use robust regression (Huber loss, RANSAC) or remove/clip extreme points.\n",
    "\n",
    "**Bonus Conclusion:**  \n",
    "Backtracking GD adapts the step size dynamically, converging faster and more reliably than a fixed learning rate.  \n",
    "It's more robust for real-world ML tasks where gradient magnitudes can vary widely.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
